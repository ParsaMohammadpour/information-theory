{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAZ8BBFc59Vr"
   },
   "source": [
    "# Project: Computing Entropy, Mutual Information, and Kullback-Leibler Divergence\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this project, you will implement three fundamental concepts from information theory: **Entropy**, **Mutual Information**, and **Kullback-Leibler (KL) Divergence**. These concepts are widely used in machine learning, data science, and statistics to measure uncertainty, information gain, and the difference between probability distributions.\n",
    "\n",
    "The project is designed to be simple yet insightful, allowing you to understand the core ideas behind these concepts by implementing them from scratch in Python. You will then apply these implementations to a real-world dataset to gain practical insights.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this project, you will:\n",
    "\n",
    "1. Understand the mathematical foundations of entropy, mutual information, and KL divergence.\n",
    "2. Implement these concepts from scratch using Python.\n",
    "3. Apply your implementations to a real-world dataset to compute these metrics and interpret the results.\n",
    "\n",
    "## Project Steps\n",
    "\n",
    "### Step 1: Implement Entropy\n",
    "\n",
    "**Entropy** is a measure of the uncertainty or randomness in a probability distribution. For a discrete random variable $ X $ with possible outcomes $ x_1, x_2, \\dots, x_n $ and corresponding probabilities $ P(x_1), P(x_2), \\dots, P(x_n) $, the entropy $ H(X) $ is defined as:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)\n",
    "$$\n",
    "\n",
    "**Task:** Write a Python function `entropy(probabilities)` that takes a list of probabilities and returns the entropy of the distribution.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def entropy(probabilities):\n",
    "    # Your code here\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Step 2: Implement Mutual Information\n",
    "\n",
    "**Mutual Information** measures the amount of information obtained about one random variable through another random variable. For two discrete random variables $ X $ and $ Y $, the mutual information $ I(X; Y) $ is defined as:\n",
    "\n",
    "$$\n",
    "I(X; Y) = \\sum_{x \\in X} \\sum_{y \\in Y} P(x, y) \\log_2 \\left( \\frac{P(x, y)}{P(x)P(y)} \\right)\n",
    "$$\n",
    "\n",
    "**Task:** Write a Python function `mutual_information(joint_prob, marginal_x, marginal_y)` that computes the mutual information between two random variables $ X $ and $ Y $.\n",
    "\n",
    "```python\n",
    "def mutual_information(joint_prob, marginal_x, marginal_y):\n",
    "    # Your code here\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Step 3: Implement Kullback-Leibler Divergence\n",
    "\n",
    "**Kullback-Leibler (KL) Divergence** measures how one probability distribution diverges from a second, reference probability distribution. For two discrete probability distributions $ P $ and $ Q $, the KL divergence $ D_{KL}(P \\| Q) $ is defined as:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\| Q) = \\sum_{i} P(i) \\log_2 \\left( \\frac{P(i)}{Q(i)} \\right)\n",
    "$$\n",
    "\n",
    "**Task:** Write a Python function `kl_divergence(p, q)` that computes the KL divergence between two probability distributions $ P $ and $ Q $.\n",
    "\n",
    "```python\n",
    "def kl_divergence(p, q):\n",
    "    # Your code here\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Step 4: Apply Your Functions to Real Data\n",
    "\n",
    "**Story:** Imagine you are a data scientist working for a botanical research institute. Your team has collected data on various iris flowers, including measurements of sepal length, sepal width, petal length, and petal width. The dataset also includes the species of each iris flower (setosa, versicolor, or virginica).\n",
    "\n",
    "Your task is to analyze this dataset using the concepts of entropy, mutual information, and KL divergence to gain insights into the relationships between the different features and the species of the flowers.\n",
    "\n",
    "**Dataset:** You will use the famous Iris dataset, which is available in the `sklearn.datasets` module.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Compute Entropy of a Feature:**\n",
    "   - Choose one of the features (e.g., sepal length) and compute its entropy. This will give you a measure of the uncertainty or randomness in the distribution of that feature.\n",
    "\n",
    "2. **Compute Mutual Information Between Two Features:**\n",
    "   - Select two features (e.g., sepal length and petal length) and compute the mutual information between them. This will tell you how much information one feature provides about the other.\n",
    "\n",
    "3. **Compute KL Divergence Between Two Distributions:**\n",
    "   - Compare the distribution of a feature (e.g., sepal length) for two different species (e.g., setosa and versicolor) using KL divergence. This will give you a measure of how different the two distributions are.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['species'] = data.target\n",
    "\n",
    "# Compute entropy for one of the features\n",
    "probabilities = df['sepal length (cm)'].value_counts(normalize=True)\n",
    "entropy_value = entropy(probabilities)\n",
    "print(f\"Entropy of sepal length: {entropy_value}\")\n",
    "\n",
    "# Compute mutual information between two features\n",
    "# (You will need to compute the joint and marginal probabilities first)\n",
    "\n",
    "# Compute KL divergence between two distributions\n",
    "# (You will need to define two probability distributions first)\n",
    "```\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. A Python script containing your implementations of `entropy`, `mutual_information`, and `kl_divergence`.\n",
    "2. A Jupyter notebook or Python script demonstrating the application of your functions to the Iris dataset.\n",
    "3. A brief report (1-2 pages) explaining your implementation, the results, and any insights gained from the project.\n",
    "\n",
    "## Grading Criteria\n",
    "\n",
    "- **Correctness (50%):** Your implementations should correctly compute entropy, mutual information, and KL divergence.\n",
    "- **Application (30%):** Your application of the functions to the Iris dataset should be meaningful and well-documented.\n",
    "- **Report (20%):** Your report should clearly explain your approach, results, and any challenges you faced.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Information Theory - Wikipedia](https://en.wikipedia.org/wiki/Information_theory)\n",
    "- [Entropy in Information Theory](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "- [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)\n",
    "- [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submit your Python script, Jupyter notebook, and report as a single zip file. Ensure that your code is well-commented and easy to follow.\n",
    "\n",
    "---\n",
    "\n",
    "Good luck, and have fun exploring the fascinating world of information theory!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEP423l65PRD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
